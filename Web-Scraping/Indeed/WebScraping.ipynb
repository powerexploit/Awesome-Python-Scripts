{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(page):\n",
    "    \"\"\" parsing  HTML page from URL \"\"\"\n",
    "    headers= {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    url= f'https://in.indeed.com/jobs?q=data+science&l=Bangalore%2C+Karnataka&start={page}'\n",
    "    r=requests.get(url,headers)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(soup):\n",
    "    \"\"\"Extracing relevant objects from page source (HTML)\"\"\"\n",
    "    divs=soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    for item in divs:\n",
    "        title=item.find('a').text.strip()\n",
    "        company= item.find('span', class_='company').text.strip()\n",
    "        try:\n",
    "            salary=item.find('span', class_= 'salaryText').text().strip()\n",
    "        except:\n",
    "            salary= ''\n",
    "        summary=item.find('div', {'class':'summary'}).text.strip().replace('\\n','')\n",
    "\n",
    "        job= {\n",
    "            'Job_Title': title,\n",
    "            'Company_Name': company,\n",
    "            'Salary': salary,\n",
    "            'Summary': summary\n",
    "        }\n",
    "        joblist.append(job)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblist=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Page 0\n",
      "Getting Page 10\n",
      "Getting Page 20\n",
      "Getting Page 30\n",
      "Getting Page 40\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,50,10):\n",
    "    print(f'Getting Page {i}')\n",
    "    c=extract(0)\n",
    "    transform(c)\n",
    "\n",
    "print(len(joblist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(joblist)\n",
    "df.head()\n",
    "df.to_csv('indeed_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
